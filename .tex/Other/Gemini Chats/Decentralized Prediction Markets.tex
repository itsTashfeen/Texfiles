\documentclass{article}
\usepackage{amsmath, amssymb, geometry, hyperref, xcolor, graphicx, listings}
\geometry{a4paper, margin=1in}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\title{Comprehensive Study Guide: Quantitative Trading Strategies in Decentralized Prediction Markets}
\author{Generated by Gemini 3 Pro}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\section{Conceptual Foundations: The Market & The Strategy}

\subsection{Market Microstructure: Binary Options on Polymarket}
The financial instruments in question are "15-minute up/down crypto polymarkets." In quantitative finance terms, these are \textbf{Binary Options}, specifically \textit{Cash-or-Nothing Call Options}.

The payoff structure is defined as:
\[
\text{Payoff} = 
\begin{cases} 
\$1 & \text{if } S_T > K \\
\$0 & \text{if } S_T \leq K 
\end{cases}
\]
Where:
\begin{itemize}
    \item $S_T$ is the spot price of the underlying asset (e.g., Bitcoin) at expiration time $T$.
    \item $K$ is the strike price (typically the spot price at the inception of the 15-minute window).
\end{itemize}

The market price of such an option, denoted as $C_{mkt}$, ranges between $\$0$ and $\$1$. This price represents the market's implied probability of the event occurring. If $C_{mkt} = 0.60$, the market consensus implies a $60\%$ probability that $S_T > K$.

\subsection{Strategic Alpha: Probabilistic vs. Latency Arbitrage}
It is crucial to distinguish between two distinct types of automated trading strategies discussed in the context of this project.

\subsubsection{Type A: Latency Arbitrage (The HyperEVM Example)}
This strategy exploits \textit{factual discrepancies} resulting from slow block times.
\begin{itemize}
    \item \textbf{Mechanism:} If a fast exchange (Hyperliquid) updates price at time $t$, but a slower L2 chain (HyperEVM) updates at $t + 2s$, the price on the L2 is stale.
    \item \textbf{Edge:} Speed. The profit is "risk-free" if the trader can execute before the block updates.
    \item \textbf{Language Requirement:} C++ or Rust is mandatory to compete for nanosecond-level execution.
\end{itemize}

\subsubsection{Type B: Probabilistic Arbitrage (The Polymarket Project)}
This strategy exploits \textit{forecast discrepancies}.
\begin{itemize}
    \item \textbf{Mechanism:} The trader builds a model to calculate a theoretical probability $P_{model}$.
    \item \textbf{Signal:} A trade is executed if the divergence between the model and the market exceeds a threshold $\epsilon$:
    \[ |P_{model} - P_{implied}| > \epsilon \]
    \item \textbf{Edge:} Intelligence (Modeling Accuracy). The profit comes from having a better volatility forecast or classification model than the market consensus.
    \item \textbf{Language Requirement:} Python is preferred. The bottleneck is model development speed and data science capabilities, not execution latency.
\end{itemize}

\section{Mathematical Framework \& Modeling}

To capture alpha in Probabilistic Arbitrage, one must derive a fair value for the binary option.

\subsection{Model A: Analytical Pricing (The Black-Scholes Baseline)}
While crypto markets often violate standard assumptions (normality of returns), the Black-Scholes-Merton (BSM) framework provides a theoretical baseline for binary options.

The value of a Cash-or-Nothing Call is simply the discounted risk-neutral probability of the option expiring in the money:
\[
C = e^{-rT} N(d_2)
\]
Where $N(\cdot)$ is the cumulative distribution function of the standard normal distribution, and $d_2$ is calculated as:
\[
d_2 = \frac{\ln(S/K) + (r - \frac{\sigma^2}{2})T}{\sigma\sqrt{T}}
\]
\begin{itemize}
    \item $\sigma$ (Volatility): The most critical input. For short-term (15-minute) options, standard daily volatility measures are insufficient. Intraday, high-frequency volatility estimation is required.
    \item $r$ (Risk-Free Rate): For $T = 15 \text{ mins}$, $r \approx 0$.
\end{itemize}

\subsection{Model B: Direct Probabilistic Classification (Machine Learning)}
A more robust approach treats the market direction as a supervised classification problem.
\[
y = f(X) + \epsilon
\]
Where $y \in \{0, 1\}$ (Down/Up) and $X$ is a vector of features.

\subsubsection{Feature Engineering}
The input vector $X$ should capture market dynamics across multiple dimensions:
\begin{enumerate}
    \item \textbf{Momentum:} RSI, MACD, Stochastic Oscillators on 1-min and 5-min timeframes.
    \item \textbf{Volatility:} Average True Range (ATR), Bollinger Band Width, Realized Volatility.
    \item \textbf{Microstructure:} Order book imbalance, bid-ask spread, trade volume delta.
\end{enumerate}

\subsubsection{The Curse of Dimensionality}
As the feature set expands (e.g., 50+ indicators), the model faces the \textit{Curse of Dimensionality}. High-dimensional space becomes sparse, leading to overfitting (learning noise) and multicollinearity.

\textbf{Solutions for Dimensionality Reduction:}
\begin{itemize}
    \item \textbf{Embedded Methods:} Using models like XGBoost or Random Forest which perform implicit feature selection via node splitting. Using L1 Regularization (Lasso) to drive coefficients of irrelevant features to zero.
    \item \textbf{Principal Component Analysis (PCA):} A linear transformation that projects data onto orthogonal axes (principal components) maximizing variance. This reduces noise and eliminates multicollinearity, though it sacrifices interpretability.
\end{itemize}

\section{Engineering \& Infrastructure}

\subsection{Technology Stack Selection}
For a probabilistic strategy, the primary goal is rapid iteration of quantitative models.
\begin{itemize}
    \item \textbf{Language:} \textbf{Python}. Leveraging the data science ecosystem (Pandas, NumPy, Scikit-Learn) outweighs the microsecond gains of C++.
    \item \textbf{Database:} \textbf{TimescaleDB} (PostgreSQL extension). Financial data is time-series data. Relational databases are necessary for structured trade logging, while time-series optimization handles tick-data ingestion.
    \item \textbf{Hosting:} Cloud VPS (e.g., AWS EC2) for 24/7 uptime and colocation near exchange servers (reducing network latency).
\end{itemize}

\section{The Execution Roadmap}

A structured, phased approach is required to mitigate risk.

\subsection{Phase 0: Feasibility Study}
\textit{Objective: Validation before construction.}
Before coding infrastructure, verify:
\begin{enumerate}
    \item \textbf{Edge Existence:} Does a manual review of historical data show mispricing $> 5\%$?
    \item \textbf{Data Availability:} Can granular order book and spot price data be accessed reliably?
    \item \textbf{Liquidity:} Is the bid-ask spread tight enough to allow entry without excessive slippage?
\end{enumerate}

\subsection{Phase 1: Modeling \& Backtesting}
\textit{Objective: Statistical significance.}
Develop the model in Python using historical data.
\begin{itemize}
    \item \textbf{Methodology:} Use \textbf{Purged K-Fold Cross-Validation}. Financial time series are correlated; standard random shuffling leaks future information into the training set. "Purging" removes data immediately following the training set to prevent leakage.
    \item \textbf{Exit Criteria:} Sharpe Ratio $> 1.5$ and consistent equity curve.
\end{itemize}

\subsection{Phase 2: Paper Trading (Forward Testing)}
\textit{Objective: System robustness.}
Deploy the bot to a live environment but log trades to a database (`paper\_trades`) instead of the exchange.
\begin{itemize}
    \item \textbf{Reconciliation:} Compare Live Paper PnL vs. Backtest PnL. Discrepancies usually stem from slippage, fees, or latency assumptions.
\end{itemize}

\subsection{Phase 3: Live Trading \& Risk Management}
\textit{Objective: Capital preservation.}
Deploy with minimal capital. Implement a "Risk Module" with veto power over the trading logic.
\begin{itemize}
    \item \textbf{Kelly Criterion (Fractional):} Use $\approx 25\%$ of full Kelly to size positions.
    \item \textbf{Circuit Breakers:} Hard stops for daily loss limits or excessive drawdowns.
\end{itemize}

\section{Strategic Career Implementation}

For candidates from non-target universities, this project serves as a "Trojan Horse" to break into quantitative finance.

\subsection{The Public Portfolio Strategy}
A public GitHub repository acts as verified proof-of-work, countering the lack of institutional brand name.
\begin{itemize}
    \item \textbf{Public Repository:} Contains the "Engine" (Data pipelines, Backtesting logic, Infrastructure code, Roadmaps).
    \item \textbf{Private Repository:} Contains the "Alpha" (Specific feature combinations, trained model weights, API keys, live strategy parameters).
\end{itemize}

This hybrid approach demonstrates professional software engineering standards and transparency without giving away the proprietary edge.

\end{document}